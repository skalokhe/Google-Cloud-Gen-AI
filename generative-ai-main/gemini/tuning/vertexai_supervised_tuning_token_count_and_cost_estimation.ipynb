{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Vertex AI Supervised Tuning token count and cost estimation.\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/vertexai_supervised_tuning_token_count_and_cost_estimation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Ftuning%2Fvertexai_supervised_tuning_token_count_and_cost_estimation.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/tuning/vertexai_supervised_tuning_token_count_and_cost_estimation.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/vertexai_supervised_tuning_token_count_and_cost_estimation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "| Author(s) | [Lehui Liu](https://github.com/liulehui), [Erwin Huizenga](https://github.com/Huize501) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook serves as a tool to preprocess and estimate token counts for tuning costs for tuning [`gemini-1.0-pro-002`](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning).\n",
    "\n",
    "\n",
    "For how to prepare dataset for tuning gemini, please refer to this [tutorial](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-about)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK and other required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform[tokenization] numpy==1.26.4 tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it's restarted, continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRvKdaPDTznN",
    "outputId": "b2519ec9-2acf-46e4-f056-ed1c615982dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## Tuning token count and cost estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPhY560YQijW"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4498u5KpQijW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import dataclasses\n",
    "import math\n",
    "\n",
    "from vertexai.generative_models import Content, Part\n",
    "from vertexai.preview.tokenization import get_tokenizer_for_model\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sgetU8OTJ7X"
   },
   "source": [
    "### Load the dataset\n",
    "\n",
    "Define the Google Cloud Storage URIs pointing to your training and validation datasets or continue using the URIs provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fikeLY4BUeDq"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "f0JwfuPSSofK"
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = \"gemini-1.0-pro-002\"  # @param ['gemini-1.0-pro-002']{type:\"string\"}\n",
    "training_dataset_uri = \"gs://cloud-samples-data/ai-platform/generative_ai/sft_train_data.jsonl\"  # @param {type:\"string\"}\n",
    "validation_dataset_uri = \"gs://cloud-samples-data/ai-platform/generative_ai/sft_validation_data.jsonl\"  # @param {type:\"string\"}\n",
    "\n",
    "tokenizer = get_tokenizer_for_model(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pHktd7HVao7"
   },
   "source": [
    "We'll now load the dataset and conduct some basic statistical analysis to understand its structure and content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTvunHqRTHqe"
   },
   "outputs": [],
   "source": [
    "with tf.io.gfile.GFile(training_dataset_uri) as dataset_jsonl_file:\n",
    "    example_training_dataset = [\n",
    "        json.loads(dataset_line) for dataset_line in dataset_jsonl_file\n",
    "    ]\n",
    "\n",
    "if validation_dataset_uri:\n",
    "    with tf.io.gfile.GFile(validation_dataset_uri) as dataset_jsonl_file:\n",
    "        example_validation_dataset = [\n",
    "            json.loads(dataset_line) for dataset_line in dataset_jsonl_file\n",
    "        ]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num training examples:\", len(example_training_dataset))\n",
    "print(\"First example:\")\n",
    "for message in example_training_dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "    print(tokenizer.count_tokens(message.get(\"content\")))\n",
    "\n",
    "if example_validation_dataset:\n",
    "    print(\"Num validation examples:\", len(example_validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ytlPXmKV3K2"
   },
   "source": [
    "### Validate the format of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cOkGZxNV4Ed"
   },
   "source": [
    "You can perform various error checks to validate that each tuning example in the dataset adheres to the format expected by the tuning API. Errors are categorized based on their nature for easier debugging.  \n",
    "  \n",
    "For how to prepare dataset for tuning gemini, please refer to this [tutorial](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-about).\n",
    "\n",
    "1. **Presence of Message List**: Checks if a `messages` list is present in each entry. Error type: `missing_messages_list`:\n",
    "2. **Message Keys Check**: Validates that each message in the messages list contains the keys `role` and `content`. Error type: `message_missing_key`.\n",
    "3. **Role Validation**: Ensures the role is one of `system`, `user`, or `model`. Error type: `unrecognized_role`. Note: only the first message can have `system` as role.\n",
    "5. **Content Validation**: Verifies that content has textual data and is a string. Error type: `missing_content`.\n",
    "6. **Consecutive Turns**. For the chat history, it is enforced that the message must can repeat in an alternating manner. Error type: `consecutive_turns`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1mzpB9PUVp5z"
   },
   "outputs": [],
   "source": [
    "def validate_dataset_format(dataset):\n",
    "    \"\"\"Validates the dataset.\n",
    "\n",
    "    Args:\n",
    "      dataset_uri: The dataset uri to be validated.\n",
    "    \"\"\"\n",
    "    format_errors = defaultdict(list)\n",
    "    if not dataset or len(dataset) == 0:\n",
    "        print(\"Input dataset file is empty or inaccessible.\")\n",
    "        return\n",
    "\n",
    "    for row_idx, example in enumerate(dataset):\n",
    "        # Verify presence of messages list\n",
    "        if not isinstance(example, dict):\n",
    "            format_errors[\"missing_messages_list\"].append(row_idx)\n",
    "            continue\n",
    "        messages = example.get(\"messages\", None)\n",
    "        try:\n",
    "            validate_messages(messages, format_errors, row_idx)\n",
    "        except (TypeError, AttributeError, KeyError) as e:\n",
    "            print(\"Invalid input during validation: %s\", e)\n",
    "            format_errors[\"invalid_input\"].append(row_idx)\n",
    "\n",
    "    if format_errors:\n",
    "        print(\"Found errors for this dataset:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"No errors found for this dataset.\")\n",
    "\n",
    "\n",
    "def validate_messages(messages, format_errors, row_index):\n",
    "    \"\"\"Validates messages list format.\"\"\"\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"].append(row_index)\n",
    "        return\n",
    "\n",
    "    # Check if the first role is for system instruction\n",
    "    if messages[0].get(\"role\", \"\").lower() == \"system\":\n",
    "        messages = messages[1:]\n",
    "    else:\n",
    "        messages = messages[:]\n",
    "\n",
    "    prev_role = None\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"].append(row_index)\n",
    "            return\n",
    "\n",
    "        if message.get(\"role\", \"\").lower() not in (\"user\", \"model\"):\n",
    "            format_errors[\"unrecognized_role\"].append(row_index)\n",
    "            return\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content:\n",
    "            format_errors[\"missing_content\"].append(row_index)\n",
    "            return\n",
    "\n",
    "            role = message.get(\"role\", \"\").lower()\n",
    "            # messages to have alternate turns.\n",
    "            if role == prev_role:\n",
    "                format_errors[\"consecutive_turns\"].append(row_index)\n",
    "                return\n",
    "\n",
    "            prev_role = role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpcJAtzhOKbv"
   },
   "source": [
    "Now you can check the data for any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUCpEmEFM0eX",
    "outputId": "15a76f55-ea26-48ce-eabe-3abae3f9b1dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found for this dataset.\n",
      "No errors found for this dataset.\n"
     ]
    }
   ],
   "source": [
    "validate_dataset_format(example_training_dataset)\n",
    "if example_validation_dataset:\n",
    "    validate_dataset_format(example_validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDlwyek4OUE2"
   },
   "source": [
    "### Utils for dataset analysis and token counting\n",
    "\n",
    "This section focuses on analyzing the structure and token counts of your datasets. You will also define some utility functions to streamline subsequent steps in the notebook.\n",
    "\n",
    "* Load and inspect sample data from the training and validation datasets.\n",
    "* Calculate token counts for messages to understand the dataset's characteristics.\n",
    "* Define utility functions for calculating token distributions and dataset statistics. These will help assess the suitability of your data for supervised tuning and estimate potential costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BgFmhH2XOdzu"
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class DatasetDistribution:\n",
    "    \"\"\"Dataset disbribution for given a population of values.\n",
    "\n",
    "    It optionally contains a histogram consists of bucketized data representing\n",
    "    the distribution of those values. The summary statistics are the sum, min,\n",
    "    max, mean, median, p5, p95.\n",
    "\n",
    "    Attributes:\n",
    "      sum: Sum of the values in the population.\n",
    "      max: Max of the values in the population.\n",
    "      min: Min of the values in the population.\n",
    "      mean: The arithmetic mean of the values in the population.\n",
    "      median: The median of the values in the population.\n",
    "      p5: P5 quantile of the values in the population.\n",
    "      p95: P95 quantile of the values in the population.\n",
    "    \"\"\"\n",
    "\n",
    "    sum: int | None = None\n",
    "    max: float | None = None\n",
    "    min: float | None = None\n",
    "    mean: float | None = None\n",
    "    median: float | None = None\n",
    "    p5: float | None = None\n",
    "    p95: float | None = None\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class DatasetStatistics:\n",
    "    \"\"\"Dataset statistics used for dataset profiling.\n",
    "\n",
    "    Attributes:\n",
    "      total_number_of_dataset_examples: Number of tuning examples in the dataset.\n",
    "      total_number_of_records_for_training: Number of tuning records after\n",
    "        formatting. Each model turn in the chat message will be considered as a record for tuning.\n",
    "      total_number_of_billable_tokens: Number of total billable tokens in the\n",
    "        dataset.\n",
    "      user_input_token_length_stats: Stats for input token length.\n",
    "      user_output_token_length_stats: Stats for output token length.\n",
    "    \"\"\"\n",
    "\n",
    "    total_number_of_dataset_examples: int | None = None\n",
    "    total_number_of_records_for_training: int | None = None\n",
    "    total_number_of_billable_tokens: int | None = None\n",
    "    user_input_token_length_stats: DatasetDistribution | None = None\n",
    "    user_output_token_length_stats: DatasetDistribution | None = None\n",
    "\n",
    "\n",
    "MAX_TOKENS_PER_EXAMPLE = 32 * 1024\n",
    "ESTIMATE_PADDING_TOKEN_PER_EXAMPLE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vm0Jwzt0RDGd"
   },
   "outputs": [],
   "source": [
    "def calculate_distribution_for_population(population) -> DatasetDistribution:\n",
    "    \"\"\"Calculates the distribution from the population of values.\n",
    "\n",
    "    Args:\n",
    "      population: The population of values to calculate distribution for.\n",
    "\n",
    "    Returns:\n",
    "      DatasetDistribution of the given population of values.\n",
    "    \"\"\"\n",
    "    if not population:\n",
    "        raise ValueError(\"population is empty\")\n",
    "\n",
    "    return DatasetDistribution(\n",
    "        sum=np.sum(population),\n",
    "        max=np.max(population),\n",
    "        min=np.min(population),\n",
    "        mean=np.mean(population),\n",
    "        median=np.median(population),\n",
    "        p5=np.percentile(population, 5, method=\"nearest\"),\n",
    "        p95=np.percentile(population, 95, method=\"nearest\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_token_distribution_for_one_tuning_dataset_example(example):\n",
    "    model_turn_token_list = []\n",
    "    input_token_list = []\n",
    "    input = []\n",
    "    n_too_long = 0\n",
    "    number_of_records_for_training = 0  # each model turn in the chat message will be considered as a record for tuning\n",
    "    for message in example[\"messages\"]:\n",
    "        role = message.get(\"role\").lower()\n",
    "        text = message.get(\"content\")\n",
    "\n",
    "        if role.lower() == \"model\":\n",
    "            result = tokenizer.count_tokens(input)\n",
    "            input_token_list.append(result.total_tokens)\n",
    "            model_turn_token_list.append(tokenizer.count_tokens(text).total_tokens)\n",
    "            number_of_records_for_training += 1\n",
    "            if (\n",
    "                result.total_tokens + tokenizer.count_tokens(text).total_tokens\n",
    "                > MAX_TOKENS_PER_EXAMPLE\n",
    "            ):\n",
    "                n_too_long += 1\n",
    "                break\n",
    "\n",
    "        input.append(Content(role=role, parts=[Part.from_text(text)]))\n",
    "\n",
    "    return (\n",
    "        input_token_list,\n",
    "        model_turn_token_list,\n",
    "        number_of_records_for_training,\n",
    "        np.sum(model_turn_token_list) + np.sum(input_token_list),\n",
    "        n_too_long,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataset_stats_for_dataset(dataset):\n",
    "    results = map(get_token_distribution_for_one_tuning_dataset_example, dataset)\n",
    "    user_input_token_list = []\n",
    "    model_turn_token_list = []\n",
    "    number_of_records_for_training = 0\n",
    "    total_number_of_billable_tokens = 0\n",
    "    n_too_long_for_dataset = 0\n",
    "    for (\n",
    "        input_token_list_per_example,\n",
    "        model_turn_token_list_per_example,\n",
    "        number_of_records_for_training_per_example,\n",
    "        number_of_billable_token_per_example,\n",
    "        n_too_long,\n",
    "    ) in results:\n",
    "        user_input_token_list.extend(input_token_list_per_example)\n",
    "        model_turn_token_list.extend(model_turn_token_list_per_example)\n",
    "        number_of_records_for_training += number_of_records_for_training_per_example\n",
    "        total_number_of_billable_tokens += number_of_billable_token_per_example\n",
    "        n_too_long_for_dataset += n_too_long\n",
    "\n",
    "    print(\n",
    "        f\"\\n{n_too_long_for_dataset} examples may be over the {MAX_TOKENS_PER_EXAMPLE} token limit, they will be truncated during tuning.\"\n",
    "    )\n",
    "\n",
    "    return DatasetStatistics(\n",
    "        total_number_of_dataset_examples=len(dataset),\n",
    "        total_number_of_records_for_training=number_of_records_for_training,\n",
    "        total_number_of_billable_tokens=total_number_of_billable_tokens\n",
    "        + number_of_records_for_training * ESTIMATE_PADDING_TOKEN_PER_EXAMPLE,\n",
    "        user_input_token_length_stats=calculate_distribution_for_population(\n",
    "            user_input_token_list\n",
    "        ),\n",
    "        user_output_token_length_stats=calculate_distribution_for_population(\n",
    "            model_turn_token_list\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def print_dataset_stats(dataset):\n",
    "    dataset_stats = get_dataset_stats_for_dataset(dataset)\n",
    "    print(\"Below you can find the dataset statistics:\")\n",
    "    print(\n",
    "        f\"Total number of examples in the dataset: {dataset_stats.total_number_of_dataset_examples}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Total number of records for training: {dataset_stats.total_number_of_records_for_training}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Total number of billable tokens in the dataset: {dataset_stats.total_number_of_billable_tokens}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"User input token length distribution: {dataset_stats.user_input_token_length_stats}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"User output token length distribution: {dataset_stats.user_output_token_length_stats}\"\n",
    "    )\n",
    "    return dataset_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSn_0r1BXo0i"
   },
   "source": [
    "Next you can analyze the structure and token counts of your datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOWsUbwVXoTU",
    "outputId": "836a1926-8f9c-40c0-be40-673c7a799acd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 examples may be over the 32768 token limit, they will be truncated during tuning.\n",
      "Below you can find the dataset statistics:\n",
      "Total number of examples in the dataset: 500\n",
      "Total number of records for training: 500\n",
      "Total number of billable tokens in the dataset: 130300\n",
      "User input token length distribution: DatasetDistribution(sum=109172, max=712, min=70, mean=218.344, median=198.5, p5=89, p95=403)\n",
      "User output token length distribution: DatasetDistribution(sum=17128, max=124, min=12, mean=34.256, median=31.0, p5=17, p95=63)\n",
      "\n",
      "0 examples may be over the 32768 token limit, they will be truncated during tuning.\n",
      "Below you can find the dataset statistics:\n",
      "Total number of examples in the dataset: 100\n",
      "Total number of records for training: 100\n",
      "Total number of billable tokens in the dataset: 28414\n",
      "User input token length distribution: DatasetDistribution(sum=23922, max=829, min=70, mean=239.22, median=225.5, p5=92, p95=430)\n",
      "User output token length distribution: DatasetDistribution(sum=3692, max=93, min=12, mean=36.92, median=36.0, p5=17, p95=63)\n"
     ]
    }
   ],
   "source": [
    "training_dataset_stats = print_dataset_stats(example_training_dataset)\n",
    "\n",
    "if example_validation_dataset:\n",
    "    validation_dataset_stats = print_dataset_stats(example_validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA6Eb-svYYN8"
   },
   "source": [
    "### Cost Estimation for Supervised Fine-tuning\n",
    "In this final section, you will estimate the total cost for supervised fine-tuning based on the number of tokens processed. The number of tokens used will be charged to you. Please refer to the [pricing page for the rate](https://cloud.google.com/vertex-ai/generative-ai/pricing#gemini-models).\n",
    "\n",
    "**Important Note:** The final cost may vary slightly from this estimate due to dataset formatting and truncation logic during training.\n",
    "\n",
    "The code calculates the total number of billable tokens by summing up the tokens from the training dataset and (if provided) the validation dataset. Then, it estimates the total cost by multiplying the total billable tokens with the number of training epochs (default is 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YwYMYE-Wrr_"
   },
   "source": [
    "### Cost estimation\n",
    "\n",
    "In this final section, you will estimate the total number of tokens used for supervised tuning. The number of tokens will be charged to you.\n",
    "\n",
    "There might be a slight difference between the estimation and actual cost due to dataset formatting and truncation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DVIpbaGYRJQc",
    "outputId": "46b2a6d4-e285-4bd0-9071-5f12a9b6d456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~158714 tokens that will be charged\n",
      "By default, you'll train for 4 epochs on this dataset.\n",
      "By default, you'll be charged for ~634856 tokens.\n"
     ]
    }
   ],
   "source": [
    "epoch_count = 4  # @param {type:\"integer\"}\n",
    "if epoch_count is None:\n",
    "    epoch_count = 4\n",
    "\n",
    "\n",
    "total_number_of_billable_tokens = training_dataset_stats.total_number_of_billable_tokens\n",
    "\n",
    "\n",
    "if validation_dataset_stats:\n",
    "    total_number_of_billable_tokens += (\n",
    "        validation_dataset_stats.total_number_of_billable_tokens\n",
    "    )\n",
    "\n",
    "print(f\"Dataset has ~{total_number_of_billable_tokens} tokens that will be charged\")\n",
    "print(f\"By default, you'll train for {epoch_count} epochs on this dataset.\")\n",
    "print(\n",
    "    f\"By default, you'll be charged for ~{epoch_count * total_number_of_billable_tokens} tokens.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "R5Xep4W9lq-Z",
    "dmWOrTJ3gx13",
    "DF4l8DTdWgPY"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
